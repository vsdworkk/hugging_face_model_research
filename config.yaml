
# Global token for all models
# hf_token: "your_token_here"

# Simplified configuration for profile analyzer
models:
  - name: "llama_3b"
    enabled: false
    model_id: "meta-llama/Llama-3.2-3B-Instruct"
    is_instruct: true
    quantization: "none"  # Options: none, 8bit, 4bit
    device_map: "auto"
    torch_dtype: "auto"
  
  - name: "llama_1b"
    enabled: true
    model_id: "meta-llama/Llama-3.2-1B-Instruct"
    is_instruct: true
    quantization: "none"
    device_map: "auto"
    torch_dtype: "auto"
  
  - name: "gemma_1b"
    enabled: true
    model_id: "google/gemma-3-1b-it"
    is_instruct: true
    quantization: "none"
    device_map: "auto"
    torch_dtype: "auto"

# Generation settings
batch_size: 10
max_new_tokens: 2000

# Data columns
input_column: "about_me"
human_label_column: "Human_flag"