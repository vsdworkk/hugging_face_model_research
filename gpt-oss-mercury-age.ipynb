{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-OSS Mercury Age Query\n",
        "\n",
        "This notebook uses OpenAI's gpt-oss-20b model to answer \"How old is the planet Mercury?\" and returns a structured JSON output with the age in years, months, weeks, and centuries.\n",
        "\n",
        "## Key Features:\n",
        "- Uses MXFP4 quantization for efficient inference on H100 GPU\n",
        "- Implements Harmony response format parsing\n",
        "- Strips reasoning/analysis tokens to show only final answer\n",
        "- Outputs structured JSON\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Installing required packages with specific versions for MXFP4 compatibility on H100 GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# Note: triton==3.4 is required for MXFP4 kernel compatibility on H100\n",
        "# The \"kernels\" package mentioned in some docs doesn't appear to be a real PyPI package\n",
        "# MXFP4 support is likely built into triton==3.4 and transformers\n",
        "%pip install -U transformers accelerate torch triton==3.4 python-dotenv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Load Hugging Face Authentication Token\n",
        "\n",
        "Loading authentication token from .env.local file for accessing Hugging Face models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env.local (if present)\n",
        "load_dotenv('.env.local')\n",
        "\n",
        "# Get Hugging Face token from either variable name\n",
        "hf_token = os.getenv('HUGGING_FACE_HUB_TOKEN') or os.getenv('HF_TOKEN')\n",
        "\n",
        "if hf_token:\n",
        "    # Mirror into both common env var names to maximize compatibility\n",
        "    os.environ['HUGGING_FACE_HUB_TOKEN'] = hf_token\n",
        "    os.environ['HF_TOKEN'] = hf_token\n",
        "\n",
        "    print(\"‚úì Hugging Face token loaded from .env.local\")\n",
        "    # Authenticate with Hugging Face (writes to local cache)\n",
        "    try:\n",
        "        from huggingface_hub import login\n",
        "        login(token=hf_token, add_to_git_credential=False)\n",
        "        print(\"‚úì Authenticated with Hugging Face\")\n",
        "    except Exception as e:\n",
        "        print(f\"WARNING: Could not perform huggingface_hub login: {e}\")\n",
        "        print(\"Proceeding with environment variable authentication only.\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: No Hugging Face token found in .env.local\")\n",
        "    print(\"   You may encounter rate limits or be unable to access gated models.\")\n",
        "    print(\"   Get your token from: https://huggingface.co/settings/tokens\")\n",
        "    print(\"   Then create a .env.local file with: HUGGING_FACE_HUB_TOKEN=your_token_here\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Verify GPU Availability\n",
        "\n",
        "Checking that H100 GPU is available and properly configured.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Verify GPU availability\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"WARNING: No GPU detected. Model will run on CPU (very slow).\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Load Model and Tokenizer\n",
        "\n",
        "Loading gpt-oss-20b with MXFP4 quantization (automatic on H100). This will download ~16GB of model weights on first run.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"openai/gpt-oss-20b\"\n",
        "\n",
        "print(f\"Loading tokenizer from {model_name}...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "print(f\"Loading model from {model_name}...\")\n",
        "print(\"This may take several minutes on first run (downloading ~16GB)...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",  # Uses MXFP4 automatically on H100\n",
        "    device_map=\"auto\"    # Automatically places model on available GPU\n",
        ")\n",
        "\n",
        "print(\"‚úì Model loaded successfully!\")\n",
        "print(f\"Model device: {model.device}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Construct Prompt with Structured Output Schema\n",
        "\n",
        "Building the prompt using the Harmony response format with:\n",
        "- System message: Defines reasoning level, channels, and model identity\n",
        "- Developer message: Includes instructions and JSON schema for structured output\n",
        "- User message: The question about Mercury's age\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Define the JSON schema for structured output\n",
        "json_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"years\": {\n",
        "            \"type\": \"number\",\n",
        "            \"description\": \"Age of Mercury in years\"\n",
        "        },\n",
        "        \"months\": {\n",
        "            \"type\": \"number\",\n",
        "            \"description\": \"Age of Mercury in months\"\n",
        "        },\n",
        "        \"weeks\": {\n",
        "            \"type\": \"number\",\n",
        "            \"description\": \"Age of Mercury in weeks\"\n",
        "        },\n",
        "        \"centuries\": {\n",
        "            \"type\": \"number\",\n",
        "            \"description\": \"Age of Mercury in centuries\"\n",
        "        }\n",
        "    },\n",
        "    \"required\": [\"years\", \"months\", \"weeks\", \"centuries\"]\n",
        "}\n",
        "\n",
        "# Construct the system message\n",
        "system_message = \"\"\"You are ChatGPT, a large language model trained by OpenAI.\n",
        "Knowledge cutoff: 2024-06\n",
        "Current date: 2025-11-08\n",
        "Reasoning: medium\n",
        "# Valid channels: analysis, commentary, final. Channel must be included for every message.\"\"\"\n",
        "\n",
        "# Construct the developer message with structured output format\n",
        "developer_message = f\"\"\"# Instructions\n",
        "Provide accurate scientific information. Output the response as valid JSON only.\n",
        "\n",
        "# Response Formats\n",
        "## mercury_age\n",
        "{json.dumps(json_schema)}\"\"\"\n",
        "\n",
        "# User question\n",
        "user_message = \"How old is the planet Mercury? Provide the age in years, months, weeks, and centuries as a JSON object.\"\n",
        "\n",
        "# Build the messages list\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": system_message},\n",
        "    {\"role\": \"developer\", \"content\": developer_message},\n",
        "    {\"role\": \"user\", \"content\": user_message}\n",
        "]\n",
        "\n",
        "print(\"Prompt constructed successfully!\")\n",
        "print(f\"\\nSystem message length: {len(system_message)} chars\")\n",
        "print(f\"Developer message length: {len(developer_message)} chars\")\n",
        "print(f\"User message: {user_message}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Generate Response\n",
        "\n",
        "Using the model's `.generate()` method with proper stop tokens to control output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply chat template and prepare inputs\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        "    return_dict=True\n",
        ").to(model.device)\n",
        "\n",
        "print(\"Generating response...\")\n",
        "print(\"This may take 30-60 seconds depending on GPU...\")\n",
        "\n",
        "# Generate with stop tokens\n",
        "# Stop tokens: <|return|> (200002) and <|call|> (200012)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=500,  # Limit response length\n",
        "    temperature=0.7,\n",
        "    eos_token_id=[200002, 200012],  # Stop at <|return|> or <|call|>\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "\n",
        "print(\"‚úì Generation complete!\")\n",
        "print(f\"Generated {len(outputs[0]) - len(inputs['input_ids'][0])} tokens\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Parse Harmony Format\n",
        "\n",
        "Extracting only the 'final' channel content while stripping reasoning/analysis tokens.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Decode the full output to see the harmony structure\n",
        "full_output = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FULL OUTPUT (with special tokens):\")\n",
        "print(\"=\"*80)\n",
        "print(full_output)\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Extract only the generated portion (after the input)\n",
        "generated_only = tokenizer.decode(outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=False)\n",
        "\n",
        "print(\"\\nGENERATED PORTION ONLY:\")\n",
        "print(\"=\"*80)\n",
        "print(generated_only)\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Parse and Display JSON Output\n",
        "\n",
        "Converting the final content to JSON and displaying it in a structured format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_json_from_text(text):\n",
        "    \"\"\"\n",
        "    Extract and parse JSON from text content.\n",
        "    Handles cases where JSON might be embedded in markdown or surrounded by text.\n",
        "    \"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    \n",
        "    # Try to parse the entire text as JSON first\n",
        "    try:\n",
        "        return json.loads(text)\n",
        "    except json.JSONDecodeError:\n",
        "        pass\n",
        "    \n",
        "    # Look for JSON block in markdown code blocks\n",
        "    json_block_pattern = r'```(?:json)?\\s*(\\{.*?\\})\\s*```'\n",
        "    matches = re.findall(json_block_pattern, text, re.DOTALL)\n",
        "    if matches:\n",
        "        try:\n",
        "            return json.loads(matches[0])\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "    \n",
        "    # Look for JSON object pattern\n",
        "    json_pattern = r'\\{[^{}]*(?:\\{[^{}]*\\}[^{}]*)*\\}'\n",
        "    matches = re.findall(json_pattern, text, re.DOTALL)\n",
        "    for match in matches:\n",
        "        try:\n",
        "            parsed = json.loads(match)\n",
        "            # Verify it has our expected keys\n",
        "            if all(key in parsed for key in ['years', 'months', 'weeks', 'centuries']):\n",
        "                return parsed\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Parse the JSON from final content\n",
        "if final_content:\n",
        "    result_json = parse_json_from_text(final_content)\n",
        "    \n",
        "    if result_json:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FINAL JSON OUTPUT - Mercury's Age:\")\n",
        "        print(\"=\"*80)\n",
        "        print(json.dumps(result_json, indent=2))\n",
        "        print(\"=\"*80)\n",
        "        \n",
        "        # Display in a more readable format\n",
        "        print(\"\\nüìä Mercury's Age Breakdown:\")\n",
        "        print(f\"   ‚Ä¢ Years:     {result_json.get('years', 'N/A'):,.0f}\")\n",
        "        print(f\"   ‚Ä¢ Months:    {result_json.get('months', 'N/A'):,.0f}\")\n",
        "        print(f\"   ‚Ä¢ Weeks:     {result_json.get('weeks', 'N/A'):,.0f}\")\n",
        "        print(f\"   ‚Ä¢ Centuries: {result_json.get('centuries', 'N/A'):,.0f}\")\n",
        "    else:\n",
        "        print(\"\\nWARNING: Could not parse JSON from final content\")\n",
        "        print(\"Final content was:\")\n",
        "        print(final_content)\n",
        "else:\n",
        "    print(\"\\nERROR: No final content to parse\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. ‚úÖ Loading OpenAI's gpt-oss-20b model with MXFP4 quantization on H100 GPU\n",
        "2. ‚úÖ Constructing prompts using the Harmony response format with structured output schema\n",
        "3. ‚úÖ Generating responses with proper stop token handling\n",
        "4. ‚úÖ Parsing the multi-channel harmony format to extract only the final (user-facing) content\n",
        "5. ‚úÖ Stripping reasoning/analysis tokens for clean JSON output\n",
        "6. ‚úÖ Displaying Mercury's age in multiple time units\n",
        "\n",
        "### Key Takeaways:\n",
        "- **MXFP4 quantization** keeps memory usage at ~16GB on H100\n",
        "- **Harmony format** separates reasoning (analysis channel) from final output (final channel)\n",
        "- **Structured output** requires defining JSON schema in the developer message\n",
        "- **Stop tokens** (<|return|>, <|call|>) control when generation should end\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
