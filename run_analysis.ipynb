{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a21d6a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "from analyzer import analyze_profiles, load_config\n",
    "from evaluate import evaluate_all_models\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e09b1d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "config = load_config('config.yaml')\n",
    "\n",
    "# Define output paths for results\n",
    "RESULTS_OUTPUT_PATH = 'profile_analysis_results.csv'  # Change this to your desired folder path\n",
    "METRICS_OUTPUT_PATH = 'model_metrics_{timestamp}.csv'  # Change this to your desired folder path\n",
    "\n",
    "# Show enabled models\n",
    "enabled_models = [m for m in config['models'] if m.get('enabled', True)]\n",
    "print(f\"Enabled models: {len(enabled_models)}\")\n",
    "for m in enabled_models:\n",
    "    print(f\"  - {m['name']}: {m['model_id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763efa5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('about_me_quality_dataset.csv')\n",
    "print(f\"Loaded {len(df)} profiles\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11753176",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f075da7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "\n",
    "results = analyze_profiles(\n",
    "    df,\n",
    "    config,  # Pass the full config\n",
    "    input_col=config.get('input_column', 'about_me'),\n",
    "    batch_size=config.get('batch_size', 10),\n",
    "    max_new_tokens=config.get('max_new_tokens', 2000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b927c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# View results\n",
    "model_names = [m['name'] for m in enabled_models]\n",
    "display_cols = ['about_me'] + [f'{name}_quality' for name in model_names]\n",
    "results[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0997efc7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate models against human labels\n",
    "if config.get('human_label_column', 'Human_flag') in results.columns:\n",
    "    # Evaluate all models and get comparison\n",
    "    comparison = evaluate_all_models(\n",
    "        results, \n",
    "        model_names,\n",
    "        true_col=config['human_label_column'],\n",
    "        print_individual_reports=False  # Set to True if you want detailed reports\n",
    "    )\n",
    "    \n",
    "    # Save metrics\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n",
    "    metrics_filename = METRICS_OUTPUT_PATH.format(timestamp=timestamp)\n",
    "    comparison.to_csv(metrics_filename, index=False)\n",
    "    print(f\"\\nModel metrics saved to: {metrics_filename}\")\n",
    "else:\n",
    "    print(\"No human labels found for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2888272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.to_csv(RESULTS_OUTPUT_PATH, index=False)\n",
    "print(f\"Results saved to {RESULTS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef051afe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Quick analysis of bad profiles\n",
    "for model_name in model_names:\n",
    "    quality_col = f'{model_name}_quality'\n",
    "    tags_col = f'{model_name}_tags'\n",
    "    \n",
    "    bad_profiles = results[results[quality_col] == 'bad']\n",
    "    print(f\"\\n{model_name}: {len(bad_profiles)} bad profiles\")\n",
    "    \n",
    "    # Count tags\n",
    "    all_tags = []\n",
    "    for tags in bad_profiles[tags_col]:\n",
    "        if isinstance(tags, list):\n",
    "            all_tags.extend(tags)\n",
    "    \n",
    "    if all_tags:\n",
    "        tag_counts = pd.Series(all_tags).value_counts()\n",
    "        print(tag_counts)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
