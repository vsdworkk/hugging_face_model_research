{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Loading Debug Notebook\n",
        "\n",
        "Simple notebook to test and debug the model loading function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import pipeline, Pipeline, BitsAndBytesConfig\n",
        "from typing import Dict, Any, Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_quantization_config(quantization: str) -> Optional[BitsAndBytesConfig]:\n",
        "    \"\"\"\n",
        "    Get quantization configuration based on string identifier.\n",
        "    \n",
        "    Args:\n",
        "        quantization: Quantization type ('none', '8bit', '4bit')\n",
        "        \n",
        "    Returns:\n",
        "        BitsAndBytesConfig object or None\n",
        "        \n",
        "    Raises:\n",
        "        ValueError: If quantization type is unknown\n",
        "    \"\"\"\n",
        "    if quantization == \"none\":\n",
        "        return None\n",
        "    elif quantization == \"8bit\":\n",
        "        return BitsAndBytesConfig(load_in_8bit=True)\n",
        "    elif quantization == \"4bit\":\n",
        "        return BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown quantization type: {quantization}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_model_pipeline(model_config: Dict[str, Any], hf_token: Optional[str] = None) -> Pipeline:\n",
        "    \"\"\"\n",
        "    Load and configure a model pipeline.\n",
        "    \n",
        "    Args:\n",
        "        model_config: Model configuration dictionary\n",
        "        hf_token: Hugging Face token for authentication\n",
        "        \n",
        "    Returns:\n",
        "        Configured text generation pipeline\n",
        "    \"\"\"\n",
        "    # Setup hub kwargs for authentication\n",
        "    hub_kwargs = {\"token\": hf_token} if hf_token else {}\n",
        "    \n",
        "    # Setup quantization and model_kwargs\n",
        "    model_kwargs = {}\n",
        "    quantization = model_config.get('quantization', 'none')\n",
        "    torch_dtype = model_config.get('torch_dtype', 'auto')\n",
        "    \n",
        "    if quantization != 'none':\n",
        "        model_kwargs['quantization_config'] = get_quantization_config(quantization)\n",
        "        # Force auto dtype when using quantization\n",
        "        torch_dtype = 'auto'\n",
        "    \n",
        "    # Create pipeline - use dtype parameter for transformers 4.55+\n",
        "    return pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_config['model_id'],\n",
        "        dtype=torch_dtype,  # Changed from torch_dtype to dtype\n",
        "        device_map=model_config.get('device_map', 'auto'),\n",
        "        model_kwargs=model_kwargs,\n",
        "        **hub_kwargs\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Configuration\n",
        "\n",
        "Edit this cell to test different model configurations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration - modify as needed\n",
        "model_config = {\n",
        "    'model_id': \"meta-llama/Llama-3.2-1B-Instruct\",\n",
        "    'quantization': \"none\",  # Options: \"none\", \"8bit\", \"4bit\"\n",
        "    'device_map': \"auto\",\n",
        "    'torch_dtype': \"auto\"\n",
        "}\n",
        "\n",
        "# Get HuggingFace token from environment (set HF_TOKEN in your .env or environment)\n",
        "hf_token = os.getenv('HF_TOKEN')\n",
        "print(f\"HF Token available: {hf_token is not None}\")\n",
        "print(f\"Model ID: {model_config['model_id']}\")\n",
        "print(f\"Quantization: {model_config['quantization']}\")\n",
        "print(f\"Device Map: {model_config['device_map']}\")\n",
        "print(f\"Torch Dtype: {model_config['torch_dtype']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n",
        "\n",
        "This is where the model loading happens - any errors will appear here:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the model pipeline\n",
        "print(\"Starting model loading...\")\n",
        "pipe = load_model_pipeline(model_config, hf_token)\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# Check memory footprint\n",
        "if hasattr(pipe.model, 'get_memory_footprint'):\n",
        "    footprint_gb = pipe.model.get_memory_footprint() / (1024 ** 3)\n",
        "    print(f\"Model memory footprint: {footprint_gb:.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Generation (Optional)\n",
        "\n",
        "Test that the model works with a simple generation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Test generation\n",
        "test_prompt = \"Hello, how are you?\"\n",
        "print(f\"Testing with prompt: '{test_prompt}'\")\n",
        "\n",
        "result = pipe(\n",
        "    test_prompt,\n",
        "    max_new_tokens=50,\n",
        "    do_sample=False,\n",
        "    return_full_text=False\n",
        ")\n",
        "\n",
        "print(\"\\nGenerated output:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "\n",
        "Free up memory when done:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "del pipe\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"GPU memory cleared\")\n",
        "else:\n",
        "    print(\"No GPU detected\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
